{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCxcINgQ4GOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZTN8t8-4HP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/drive/My Drive/Deep_Learning_Codes/CNN_model_restore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv8KUpNk4Ncs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_deT0WDU367p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(clases, caracteristicas):\n",
        "  (X_train, Y_tr), (X_test, Y_te) = cifar10.load_data()\n",
        "  \n",
        "  Train_samples=Y_tr.size\n",
        "  Y_train=np.zeros((Train_samples,clases))\n",
        "  for i in range(Train_samples):\n",
        "      Y_train[i,Y_tr[i]]=1    \n",
        "\n",
        "  Test_samples=Y_te.size\n",
        "  Y_test=np.zeros((Test_samples,clases))\n",
        "  for i in range(Test_samples):\n",
        "      Y_test[i,Y_te[i]]=1\n",
        "  return (X_train/255,X_test/255,Y_train,Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y34Ljg1I4jjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_placeholders(n_input,n_classes):\n",
        "    X = tf.placeholder(\"float\", [None, n_input[0],n_input[1],n_input[2]])\n",
        "    Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "    return (X,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhtA0dVA5DQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(n_input, K_size_1, K_size_2, K_size_3, N_filters_1, N_filters_2, N_filters_3):\n",
        "    W1 = tf.get_variable(\"W1\", [K_size_1, K_size_1, n_input[2], N_filters_1], initializer = \n",
        "    \t\t\t\t\t\t\ttf.contrib.layers.xavier_initializer(seed = 0))\n",
        "    W2 = tf.get_variable(\"W2\", [K_size_2, K_size_2, N_filters_1, N_filters_2], initializer = \n",
        "    \t\t\t\t\t\t\ttf.contrib.layers.xavier_initializer(seed = 0))\n",
        "    W3 = tf.get_variable(\"W3\", [K_size_3, K_size_3, N_filters_2, N_filters_3], initializer = \n",
        "    \t\t\t\t\t\t\ttf.contrib.layers.xavier_initializer(seed = 0))\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"W2\": W2,\n",
        "                  \"W3\": W3,}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFjZckDP5c91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters, n_classes):\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    W3 = parameters['W3']\n",
        "    #Definicion capas convolucionales\n",
        "    A1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME'))\n",
        "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
        "    A2 = tf.nn.relu(tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME'))\n",
        "    P2 = tf.nn.max_pool(A2, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
        "    A3 = tf.nn.relu(tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding = 'SAME'))\n",
        "    F1 = tf.contrib.layers.flatten(A3)\n",
        "    # Definición del perceptrón multicapa\n",
        "    F2 = tf.contrib.layers.fully_connected(F1, 128, None)\n",
        "    out_layer = tf.contrib.layers.fully_connected(F2, n_classes, None)\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckpsaYRF5-5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(logits, Y):\n",
        "    loss_op=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "    return loss_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcMEesop6QS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(training_epochs, batch_size, display_step):\n",
        "    #Objeto para el almacenamiento de los pesos\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        # Ciclo en el que se repetirá el proceso de entrenamiento\n",
        "        for epoch in range(training_epochs):\n",
        "            avg_cost = 0.\n",
        "            #obtiene el numero de grupos en que queda dividida la base de datos\n",
        "            total_batch = int(Y_train.shape[0]/batch_size) \n",
        "            # ciclo para entrenar con cada grupo de datos\n",
        "            for i in range(total_batch-1):\n",
        "                batch_x= X_train[i*batch_size:(i+1)*batch_size]\n",
        "                batch_y= Y_train[i*batch_size:(i+1)*batch_size]\n",
        "                # Correr la funcion de perdida y la operacion de optimización \n",
        "                # con la respectiva alimentación del placeholder\n",
        "                _,c =sess.run([train_op, cost],feed_dict={X:batch_x,Y:batch_y})\n",
        "                # Promedio de resultados de la funcion de pérdida\n",
        "                avg_cost += c / total_batch\n",
        "            # Mostrar el resultado del entrenamiento por grupos\n",
        "            if epoch % display_step == 0:\n",
        "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
        "        print(\"Optimization Finished!\")\n",
        "        saver.save(sess, \"saved_model/model.ckpt\")\n",
        "        # Calculo de presición\n",
        "        traa=0.\n",
        "        for i in range(total_batch-1):\n",
        "            batch_x= X_train[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y= Y_train[i*batch_size:(i+1)*batch_size]\n",
        "            acc=sess.run(accuracy,feed_dict={X:batch_x,Y:batch_y})\n",
        "            traa += acc / total_batch\n",
        "        teaa=0.\n",
        "        total_batch = int(Y_test.shape[0]/batch_size) \n",
        "        for i in range(total_batch-1):\n",
        "            batch_x= X_test[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y= Y_test[i*batch_size:(i+1)*batch_size]\n",
        "            acc=sess.run(accuracy,feed_dict={X: batch_x, Y: batch_y})\n",
        "            teaa += acc / total_batch\n",
        "        print(\"Train_Accuracy:\", acc)\n",
        "        print(\"Test_Accuracy:\", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfQUiPDp8FUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "11910a9e-5461-48f4-9aa9-2b008cb53078"
      },
      "source": [
        "tf.reset_default_graph() #Linea para eliminar el grafo que se encuentra en memoria\n",
        "\n",
        "# Parametros de entrenamiento\n",
        "learning_rate = 0.001\n",
        "training_epochs = 5\n",
        "batch_size = 256\n",
        "display_step = 1\n",
        "\n",
        "# Parámetros para la contrucción del modelo\n",
        "K_size_1 = 5 # tamaño del Kernel (Ej:5*5)\n",
        "K_size_2 = 5 \n",
        "K_size_3 = 3 \n",
        "N_filters_1 = 6\n",
        "N_filters_2 = 12\n",
        "N_filters_3 = 24\n",
        "n_input = (32,32,3) # Caracteristicas de entrada (ej:28*28 pixeles (mnist))\n",
        "n_classes = 10 # Total de clases en la base de datos\n",
        "\n",
        "#Cargar la base de datos para entrenar\n",
        "X_train,X_test,Y_train,Y_test=load_data(n_classes, n_input)\n",
        "\n",
        "# CONSTRUCCION DEL GRAFO\n",
        "################################################################################\n",
        "# Crear placeholders\n",
        "X,Y = create_placeholders(n_input,n_classes)\n",
        "# Definir parámetros\n",
        "parameters = initialize_parameters(n_input, K_size_1, K_size_2, K_size_3, N_filters_1, N_filters_2, N_filters_3)\n",
        "\n",
        "# Declarar la operación que aplica el MLP usando la información de entrada\n",
        "logits = forward_propagation(X, parameters, n_classes)\n",
        "\n",
        "# Declarar las operaciónes que establecen la funcion de costo y optimización\n",
        "cost = compute_cost(logits, Y)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(cost)\n",
        "\n",
        "# Declarar operaciones para el calculo de la precisión \n",
        "pred_onehot = tf.nn.softmax(logits)  # aplicando softmax al modelo\n",
        "prediccion= tf.argmax(pred_onehot, 1)\n",
        "correct_prediction = tf.equal(prediccion, tf.argmax(Y, 1)) #comparando cada salida de la red con las etiquetas\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))# (prediciones correctas)/(totales)\n",
        "\n",
        "\n",
        "# inicializador de variables\n",
        "init = tf.global_variables_initializer()\n",
        "################################################################################\n",
        "\n",
        "# Inicio de sesion para la ejecusión del Grafo\n",
        "train(training_epochs, batch_size, display_step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-8-caf7fd9a590a>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost=2.021045747\n",
            "Epoch: 0002 cost=1.730590882\n",
            "Epoch: 0003 cost=1.653063681\n",
            "Epoch: 0004 cost=1.599383724\n",
            "Epoch: 0005 cost=1.562964313\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5uk7Jbt8HYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CODIGO PARA USAR EL MODELO PARA LA INFERENCIA\n",
        "def model_predict(data_test):\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, \"saved_model/model.ckpt\")        \n",
        "        print(\"Model restored.\")\n",
        "        pred=sess.run([prediccion], feed_dict={X: data_test})\n",
        "        return pred\n",
        "\n",
        "for indice in range (Y_test.shape[0]):\n",
        "    pred=model_predict([X_test[indice]])\n",
        "    print(\"\\n\\nreal:\", np.argmax(Y_test[indice]))\n",
        "    print(\"prediccion:\", pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}